1. Introduction
This project implements baseline LSTM and attention-based encoder-decoder models for time series forecasting.

2. Dataset
Synthetic multivariate time series with trend, seasonality, and noise.

3. Model Architecture
Baseline LSTM and Attention-based Encoder-Decoder LSTM with attention mechanism.

4. Hyperparameter Strategy
We experimented with sequence lengths (10,20), LSTM units (32,64), and learning rates (0.001,0.0005).

5. Metrics
MAE, RMSE, and MAPE were used for evaluation.

6. Results
Attention-based model showed improved performance over baseline LSTM.

7. Attention Interpretability
Attention weights highlight important time steps contributing to predictions.

8. Conclusion
Attention mechanisms improve long-term dependency learning in time series.
